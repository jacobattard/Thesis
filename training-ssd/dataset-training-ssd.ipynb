{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6f7da1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models.detection import ssd300_vgg16\n",
    "from torchvision.transforms import functional as F\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "from PIL import Image\n",
    "import os\n",
    "import json\n",
    "import copy\n",
    "from roboflow import Roboflow\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8766dd16",
   "metadata": {},
   "source": [
    "# Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "380300b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading Dataset Version Zip in handball-detection-8 to coco:: 100%|██████████| 469233/469233 [00:28<00:00, 16484.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting Dataset Version Zip to handball-detection-8 in coco:: 100%|██████████| 2329/2329 [00:01<00:00, 1633.98it/s]\n"
     ]
    }
   ],
   "source": [
    "rf = Roboflow(api_key=\"QmzA8vyVJAsptHIaUGx5\")\n",
    "project = rf.workspace(\"penalty-detection\").project(\"handball-detection-op71z\")\n",
    "version = project.version(8)\n",
    "dataset = version.download(\"coco\")\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e72df9e",
   "metadata": {},
   "source": [
    "# Loading Dataset (COCO format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9dc394fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.02s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "class CocoDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, images_dir, annotations_file, transforms=None):\n",
    "        self.images_dir = images_dir\n",
    "        self.coco = COCO(annotations_file)\n",
    "        self.img_ids = self.coco.getImgIds()\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.img_ids[idx]\n",
    "        img_info = self.coco.loadImgs(img_id)[0]\n",
    "        img_path = os.path.join(self.images_dir, img_info['file_name'])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        img_tensor = F.to_tensor(img)\n",
    "\n",
    "        ann_ids = self.coco.getAnnIds(imgIds=img_id)\n",
    "        anns = self.coco.loadAnns(ann_ids)\n",
    "\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        for ann in anns:\n",
    "            x, y, w, h = ann['bbox']\n",
    "            boxes.append([x, y, x+w, y+h])\n",
    "            labels.append(ann['category_id'])\n",
    "        \n",
    "        target = {\n",
    "            \"boxes\": torch.tensor(boxes, dtype=torch.float32),\n",
    "            \"labels\": torch.tensor(labels, dtype=torch.int64)\n",
    "        }\n",
    "\n",
    "        if self.transforms:\n",
    "            img_tensor = self.transforms(img_tensor)\n",
    "        \n",
    "        return img_tensor, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_ids)\n",
    "\n",
    "# Paths\n",
    "TRAIN_IMAGES = \"handball-detection-8/train\"\n",
    "TRAIN_ANNOTATIONS = \"handball-detection-8/train/_annotations.coco.json\"\n",
    "VAL_IMAGES = \"handball-detection-8/valid\"\n",
    "VAL_ANNOTATIONS = \"handball-detection-8/valid/_annotations.coco.json\"\n",
    "\n",
    "# Dataset and DataLoader\n",
    "train_dataset = CocoDataset(TRAIN_IMAGES, TRAIN_ANNOTATIONS)\n",
    "val_dataset = CocoDataset(VAL_IMAGES, VAL_ANNOTATIONS)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6601024f",
   "metadata": {},
   "source": [
    "# Setting up Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81efc771",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=SSD300_VGG16_Weights.COCO_V1`. You can also use `weights=SSD300_VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "Downloading: \"https://download.pytorch.org/models/ssd300_vgg16_coco-b556d3b4.pth\" to C:\\Users\\Jacob/.cache\\torch\\hub\\checkpoints\\ssd300_vgg16_coco-b556d3b4.pth\n",
      "100%|██████████| 136M/136M [00:08<00:00, 16.6MB/s] \n"
     ]
    }
   ],
   "source": [
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "NUM_CLASSES = 3  # including background\n",
    "\n",
    "model = ssd300_vgg16(pretrained=True)\n",
    "model.head.classification_head.num_classes = NUM_CLASSES\n",
    "model.to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5708035",
   "metadata": {},
   "source": [
    "# Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f366d924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Loss: 2.6357\n",
      "Validation Loss: 2.1991\n",
      "Epoch [2/200], Loss: 2.2189\n",
      "Validation Loss: 1.8472\n",
      "Epoch [3/200], Loss: 1.9325\n",
      "Validation Loss: 1.6412\n",
      "Epoch [4/200], Loss: 1.7673\n",
      "Validation Loss: 1.5740\n",
      "Epoch [5/200], Loss: 1.6775\n",
      "Validation Loss: 1.5304\n",
      "Epoch [6/200], Loss: 1.5605\n",
      "Validation Loss: 1.4344\n",
      "Epoch [7/200], Loss: 1.4353\n",
      "Validation Loss: 1.3649\n",
      "Epoch [8/200], Loss: 1.4146\n",
      "Validation Loss: 1.4523\n",
      "Epoch [9/200], Loss: 1.5537\n",
      "Validation Loss: 1.3689\n",
      "Epoch [10/200], Loss: 1.3102\n",
      "Validation Loss: 1.3334\n",
      "Epoch [11/200], Loss: 1.2056\n",
      "Validation Loss: 1.2886\n",
      "Epoch [12/200], Loss: 1.1636\n",
      "Validation Loss: 1.3083\n",
      "Epoch [13/200], Loss: 1.4899\n",
      "Validation Loss: 1.3925\n",
      "Epoch [14/200], Loss: 1.1556\n",
      "Validation Loss: 1.2645\n",
      "Epoch [15/200], Loss: 1.1297\n",
      "Validation Loss: 1.1624\n",
      "Epoch [16/200], Loss: 1.0327\n",
      "Validation Loss: 1.1990\n",
      "Epoch [17/200], Loss: 1.9788\n",
      "Validation Loss: 1.5875\n",
      "Epoch [18/200], Loss: 1.4660\n",
      "Validation Loss: 1.2681\n",
      "Epoch [19/200], Loss: 1.1193\n",
      "Validation Loss: 1.2607\n",
      "Epoch [20/200], Loss: 1.0167\n",
      "Validation Loss: 1.2985\n",
      "Epoch [21/200], Loss: 0.9116\n",
      "Validation Loss: 1.1941\n",
      "Epoch [22/200], Loss: 0.8906\n",
      "Validation Loss: 1.2039\n",
      "Epoch [23/200], Loss: 0.8128\n",
      "Validation Loss: 1.2748\n",
      "Epoch [24/200], Loss: 0.8542\n",
      "Validation Loss: 1.2320\n",
      "Epoch [25/200], Loss: 3.4066\n",
      "Validation Loss: 4.4707\n",
      "Early stopping triggered.\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 200\n",
    "PATIENCE = 10\n",
    "\n",
    "best_model_wts = deepcopy(model.state_dict())\n",
    "best_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for images, targets in train_loader:\n",
    "        images = list(img.to(DEVICE) for img in images)\n",
    "        targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss_dict = model(images, targets)\n",
    "        loss = sum(loss for loss in loss_dict.values())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}], Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    # Validation loss for early stopping\n",
    "    model.train()  # <-- keep in training mode for loss computation\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, targets in val_loader:\n",
    "            images = list(img.to(DEVICE) for img in images)\n",
    "            targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            loss_dict = model(images, targets)  # returns dict of losses\n",
    "            loss = sum(loss for loss in loss_dict.values())\n",
    "            val_loss += loss.item()\n",
    "    val_loss /= len(val_loader)\n",
    "    print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # Early stopping\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        best_model_wts = deepcopy(model.state_dict())\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= PATIENCE:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# Load best model weights\n",
    "model.load_state_dict(best_model_wts)\n",
    "torch.save(model.state_dict(), \"ssd_best_model.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0b01a9",
   "metadata": {},
   "source": [
    "# Evaluation Metrics (Precision, Recall, mAP@0.5, mAP@0.5:0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fbf83f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.04s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.01s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.568\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.852\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.620\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.114\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.502\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.761\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.508\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.660\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.660\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.264\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.589\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.798\n"
     ]
    }
   ],
   "source": [
    "# Paths\n",
    "TEST_IMAGES_DIR = 'handball-detection-8/test'\n",
    "TEST_ANNOTATIONS = 'handball-detection-8/test/_annotations.coco.json'\n",
    "CONF_THRESHOLD = 0.05\n",
    "\n",
    "coco = COCO(TEST_ANNOTATIONS)\n",
    "img_ids = coco.getImgIds()\n",
    "\n",
    "predictions = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for img_id in img_ids:\n",
    "        img_info = coco.loadImgs(img_id)[0]\n",
    "        img_path = os.path.join(TEST_IMAGES_DIR, img_info['file_name'])\n",
    "        image = F.to_tensor(Image.open(img_path).convert(\"RGB\")).to(DEVICE).unsqueeze(0)\n",
    "\n",
    "        outputs = model(image)[0]\n",
    "        keep = outputs['scores'] > CONF_THRESHOLD\n",
    "        boxes = outputs['boxes'][keep]\n",
    "        labels = outputs['labels'][keep]\n",
    "        scores = outputs['scores'][keep]\n",
    "\n",
    "        for box, label, score in zip(boxes, labels, scores):\n",
    "            x_min, y_min, x_max, y_max = box.tolist()\n",
    "            predictions.append({\n",
    "                'image_id': img_id,\n",
    "                'category_id': int(label),\n",
    "                'bbox': [x_min, y_min, x_max - x_min, y_max - y_min],\n",
    "                'score': float(score)\n",
    "            })\n",
    "\n",
    "# COCO Evaluation\n",
    "coco_pred = coco.loadRes(predictions)\n",
    "coco_eval = COCOeval(coco, coco_pred, 'bbox')\n",
    "coco_eval.evaluate()\n",
    "coco_eval.accumulate()\n",
    "coco_eval.summarize()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
